{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "np.nan is an invalid document, expected byte or unicode string.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 56\u001b[0m\n\u001b[0;32m     53\u001b[0m movies_df \u001b[38;5;241m=\u001b[39m load_movie_data(movies_data_path)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Build the recommendation model\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m similarity_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_recommendation_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmovies_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Input movies for recommendations\u001b[39;00m\n\u001b[0;32m     59\u001b[0m input_movies \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVenom: The Last Dance\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe Wild Robot\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWicked\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "Cell \u001b[1;32mIn[6], line 22\u001b[0m, in \u001b[0;36mbuild_recommendation_model\u001b[1;34m(movies_df)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Convert metadata into TF-IDF features\u001b[39;00m\n\u001b[0;32m     21\u001b[0m tfidf \u001b[38;5;241m=\u001b[39m TfidfVectorizer(stop_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 22\u001b[0m tfidf_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mtfidf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmovies_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Calculate similarity matrix\u001b[39;00m\n\u001b[0;32m     25\u001b[0m similarity_matrix \u001b[38;5;241m=\u001b[39m linear_kernel(tfidf_matrix, tfidf_matrix)\n",
      "File \u001b[1;32mc:\\Users\\arsen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2138\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2131\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[0;32m   2132\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2133\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[0;32m   2134\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[0;32m   2135\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[0;32m   2136\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[0;32m   2137\u001b[0m )\n\u001b[1;32m-> 2138\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2140\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2141\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\arsen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\arsen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1389\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1381\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1382\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1384\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1385\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1386\u001b[0m             )\n\u001b[0;32m   1387\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1389\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1392\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\arsen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1276\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1274\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[0;32m   1275\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m-> 1276\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1277\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1278\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32mc:\\Users\\arsen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:105\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Chain together an optional series of text processing steps to go from\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;124;03ma single document to ngrams, with or without tokenizing or preprocessing.\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;124;03m    A sequence of tokens, possibly with pairs, triples, etc.\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m decoder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 105\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m analyzer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    107\u001b[0m     doc \u001b[38;5;241m=\u001b[39m analyzer(doc)\n",
      "File \u001b[1;32mc:\\Users\\arsen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:238\u001b[0m, in \u001b[0;36m_VectorizerMixin.decode\u001b[1;34m(self, doc)\u001b[0m\n\u001b[0;32m    235\u001b[0m     doc \u001b[38;5;241m=\u001b[39m doc\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode_error)\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m doc \u001b[38;5;129;01mis\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnan:\n\u001b[1;32m--> 238\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    239\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp.nan is an invalid document, expected byte or unicode string.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    240\u001b[0m     )\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m doc\n",
      "\u001b[1;31mValueError\u001b[0m: np.nan is an invalid document, expected byte or unicode string."
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "# import numpy as np\n",
    "\n",
    "# # Load the cleaned dataset\n",
    "# data = pd.read_csv('movies_data_cleaned.csv')\n",
    "\n",
    "# # 1. Data Preparation\n",
    "# # Combine genres into a single string for each movie\n",
    "# genre_columns = data.columns[6:-1]  # Exclude unnecessary columns\n",
    "# data['combined_genres'] = data[genre_columns].apply(lambda row: ' '.join(row[row == 1].index), axis=1)\n",
    "\n",
    "# # 2. Feature Engineering\n",
    "# # TF-IDF Vectorization of descriptions\n",
    "# tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "# tfidf_matrix = tfidf_vectorizer.fit_transform(data['description'].fillna(''))\n",
    "\n",
    "# # Combine TF-IDF features with genres\n",
    "# data['combined_metadata'] = data['combined_genres'] + ' ' + data['description']\n",
    "# combined_vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "# combined_matrix = combined_vectorizer.fit_transform(data['combined_metadata'])\n",
    "\n",
    "# # 3. Similarity Calculation\n",
    "# # Compute pairwise cosine similarity\n",
    "# cosine_sim = cosine_similarity(combined_matrix)\n",
    "\n",
    "# # 4. Recommendation Function\n",
    "# def recommend_movies(movie_title, cosine_sim=cosine_sim, data=data, top_n=5):\n",
    "#     \"\"\"\n",
    "#     Recommend movies similar to a given movie based on cosine similarity.\n",
    "#     :param movie_title: Title of the movie to base recommendations on\n",
    "#     :param cosine_sim: Precomputed cosine similarity matrix\n",
    "#     :param data: Dataset containing movies\n",
    "#     :param top_n: Number of recommendations to return\n",
    "#     :return: List of recommended movies\n",
    "#     \"\"\"\n",
    "#     # Find the index of the movie\n",
    "#     idx = data[data['title'] == movie_title].index[0]\n",
    "    \n",
    "#     # Get similarity scores for all movies with the given movie\n",
    "#     sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    \n",
    "#     # Sort movies by similarity score\n",
    "#     sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "#     # Get top_n movies excluding the input movie itself\n",
    "#     top_movies = [data.iloc[i[0]]['title'] for i in sim_scores[1:top_n + 1]]\n",
    "    \n",
    "#     return top_movies\n",
    "\n",
    "# # Test the recommendation function\n",
    "# test_movie = \"Transmorphers: Mech Beasts\"\n",
    "# print(f\"Movies similar to '{test_movie}':\")\n",
    "# print(recommend_movies(test_movie))\n",
    "\n",
    "# # 5. Evaluation\n",
    "# # Simulate a ground truth by assuming movies with the same genres are good recommendations\n",
    "# def evaluate_recommendation_system(data, cosine_sim, top_n=5):\n",
    "#     \"\"\"\n",
    "#     Evaluate the recommendation system using simulated ground truth.\n",
    "#     :param data: Dataset containing movies\n",
    "#     :param cosine_sim: Precomputed cosine similarity matrix\n",
    "#     :param top_n: Number of recommendations to return\n",
    "#     :return: Evaluation metrics (MAE, RMSE)\n",
    "#     \"\"\"\n",
    "#     true_ratings = []\n",
    "#     predicted_ratings = []\n",
    "    \n",
    "#     for idx, row in data.iterrows():\n",
    "#         sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "#         sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "#         # Get top_n recommendations\n",
    "#         recommendations = [data.iloc[i[0]]['rating'] for i in sim_scores[1:top_n + 1]]\n",
    "        \n",
    "#         # Use the current movie's rating as ground truth for evaluation\n",
    "#         true_rating = row['rating']\n",
    "        \n",
    "#         # Simulate predicted ratings as the mean of the recommended movies' ratings\n",
    "#         predicted_rating = np.mean(recommendations)\n",
    "        \n",
    "#         true_ratings.append(true_rating)\n",
    "#         predicted_ratings.append(predicted_rating)\n",
    "    \n",
    "#     # Compute MAE and RMSE\n",
    "#     mae = mean_absolute_error(true_ratings, predicted_ratings)\n",
    "#     rmse = mean_squared_error(true_ratings, predicted_ratings, squared=False)\n",
    "    \n",
    "#     return mae, rmse\n",
    "\n",
    "# # Evaluate the model\n",
    "# mae, rmse = evaluate_recommendation_system(data, cosine_sim)\n",
    "# print(f\"Evaluation Results - MAE: {mae:.4f}, RMSE: {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Movies: ['Wicked: Part Two', 'Venom', 'Robot Dreams', 'Venom: Let There Be Carnage', 'The Adventure of A.R.I.: My Robot Friend', 'The Wizard of Oz', 'Palmer', 'Supergirl', 'I, Robot', 'Journey 2: The Mysterious Island']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arsen\\AppData\\Local\\Temp\\ipykernel_14148\\1218353534.py:68: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  user_movie_ratings = user_movie_ratings.fillna(0)  # Replace NaN with 0 for unrated movies\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "boolean index did not match indexed array along dimension 1; dimension is 1 but corresponding boolean dimension is 4028",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 112\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecommended Movies:\u001b[39m\u001b[38;5;124m\"\u001b[39m, recommendations)\n\u001b[0;32m    111\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m--> 112\u001b[0m mae, rmse \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmovies_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msimilarity_matrix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel Evaluation:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMean Absolute Error (MAE): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmae\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[15], line 87\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(movies_df, similarity_matrix)\u001b[0m\n\u001b[0;32m     85\u001b[0m true_indices \u001b[38;5;241m=\u001b[39m test_data \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# Boolean mask for rated movies\u001b[39;00m\n\u001b[0;32m     86\u001b[0m y_true \u001b[38;5;241m=\u001b[39m test_data[true_indices]\n\u001b[1;32m---> 87\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mpredicted_ratings\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrue_indices\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     89\u001b[0m mae \u001b[38;5;241m=\u001b[39m mean_absolute_error(y_true, y_pred)\n\u001b[0;32m     90\u001b[0m rmse \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(mean_squared_error(y_true, y_pred))\n",
      "\u001b[1;31mIndexError\u001b[0m: boolean index did not match indexed array along dimension 1; dimension is 1 but corresponding boolean dimension is 4028"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "def load_movie_data(file_path):\n",
    "    \"\"\"\n",
    "    Load movie data from a CSV file.\n",
    "    \"\"\"\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "def build_recommendation_model(movies_df):\n",
    "    \"\"\"\n",
    "    Build a content-based recommendation model using movie metadata.\n",
    "    \"\"\"\n",
    "    # Combine relevant metadata for better similarity calculations\n",
    "    movies_df['metadata'] = (movies_df['genres'] + \" \" + \n",
    "                             movies_df['description'] + \" \" + \n",
    "                             movies_df['title'])\n",
    "    \n",
    "    # Fill NaN values with an empty string\n",
    "    movies_df['metadata'] = movies_df['metadata'].fillna(\"\")\n",
    "\n",
    "    # Convert metadata into TF-IDF features\n",
    "    tfidf = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = tfidf.fit_transform(movies_df['metadata'])\n",
    "\n",
    "    # Calculate similarity matrix\n",
    "    similarity_matrix = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "    return similarity_matrix\n",
    "\n",
    "def get_recommendations(input_titles, movies_df, similarity_matrix, num_recommendations=10):\n",
    "    \"\"\"\n",
    "    Provide recommendations based on the input titles.\n",
    "    \"\"\"\n",
    "    # Find indices of input movies\n",
    "    movie_indices = [movies_df[movies_df['title'] == title].index[0] for title in input_titles if title in movies_df['title'].values]\n",
    "\n",
    "    # Aggregate similarity scores for input movies\n",
    "    sim_scores = similarity_matrix[movie_indices].sum(axis=0)\n",
    "    sim_scores = [(i, score) for i, score in enumerate(sim_scores)]\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Filter out input movies from recommendations\n",
    "    input_indices = set(movie_indices)\n",
    "    recommendations = [(i, score) for i, score in sim_scores if i not in input_indices]\n",
    "\n",
    "    # Get top recommendations\n",
    "    top_recommendations = recommendations[:num_recommendations]\n",
    "    recommended_titles = [movies_df.iloc[i]['title'] for i, _ in top_recommendations]\n",
    "\n",
    "    return recommended_titles\n",
    "\n",
    "def evaluate_model(movies_df, similarity_matrix):\n",
    "    \"\"\"\n",
    "    Evaluate the recommendation model using MAE and RMSE.\n",
    "    \"\"\"\n",
    "    # Create a simulated ratings matrix for evaluation\n",
    "    user_movie_ratings = pd.DataFrame(index=range(len(movies_df)), columns=range(len(movies_df)))\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    for user in range(len(movies_df)):\n",
    "        # Assign random ratings for some movies\n",
    "        rated_movies = np.random.choice(len(movies_df), size=np.random.randint(5, 15), replace=False)\n",
    "        user_movie_ratings.loc[user, rated_movies] = np.random.uniform(1, 5, size=len(rated_movies))\n",
    "    \n",
    "    user_movie_ratings = user_movie_ratings.fillna(0)  # Replace NaN with 0 for unrated movies\n",
    "\n",
    "    # Train-test split\n",
    "    train_data, test_data = train_test_split(user_movie_ratings, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Convert dataframes to NumPy arrays for calculations\n",
    "    train_data = train_data.to_numpy()\n",
    "    test_data = test_data.to_numpy()\n",
    "\n",
    "    # Predict ratings for the test set\n",
    "    predicted_ratings = []\n",
    "    for user_ratings in test_data:\n",
    "        pred_ratings = similarity_matrix.dot(user_ratings) / np.array([np.abs(similarity_matrix).sum(axis=1)])\n",
    "        predicted_ratings.append(pred_ratings)\n",
    "    predicted_ratings = np.array(predicted_ratings)\n",
    "\n",
    "    # Filter true and predicted ratings to match dimensions\n",
    "    true_indices = test_data > 0  # Boolean mask for rated movies\n",
    "    y_true = test_data[true_indices]\n",
    "    y_pred = predicted_ratings[true_indices]\n",
    "\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "    return mae, rmse\n",
    "\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "# Load movie data\n",
    "movies_data_path = \"movies_data_cleaned.csv\"  # Replace with your file path\n",
    "movies_df = load_movie_data(movies_data_path)\n",
    "\n",
    "# Build the recommendation model\n",
    "similarity_matrix = build_recommendation_model(movies_df)\n",
    "\n",
    "# Input movies for recommendations\n",
    "input_movies = [\"Venom: The Last Dance\", \"The Wild Robot\", \"Wicked\"]\n",
    "\n",
    "# Get recommendations\n",
    "recommendations = get_recommendations(input_movies, movies_df, similarity_matrix)\n",
    "print(\"Recommended Movies:\", recommendations)\n",
    "\n",
    "# Evaluate the model\n",
    "mae, rmse = evaluate_model(movies_df, similarity_matrix)\n",
    "print(\"Model Evaluation:\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
